# =============================================================================
# GenBI Configuration - Ollama Local Setup
# =============================================================================
# This configuration uses Ollama for completely local and free LLM inference

# -----------------------------------------------------------------------------
# API Server Settings
# -----------------------------------------------------------------------------
API_HOST=0.0.0.0
API_PORT=5556

# -----------------------------------------------------------------------------
# LLM Configuration - Ollama (Local)
# -----------------------------------------------------------------------------
LLM_PROVIDER=openai
LLM_MODEL=llama3.1:8b
LLM_API_KEY=ollama
LLM_BASE_URL=http://localhost:11434/v1
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=2000

# -----------------------------------------------------------------------------
# Embedding Configuration - Ollama (Local)
# -----------------------------------------------------------------------------
EMBEDDING_PROVIDER=openai
EMBEDDING_MODEL=nomic-embed-text
EMBEDDING_API_KEY=ollama
EMBEDDING_BASE_URL=http://localhost:11434/v1

# -----------------------------------------------------------------------------
# Vector Store Settings
# -----------------------------------------------------------------------------
VECTOR_STORE_PATH=./data/vector_store

# -----------------------------------------------------------------------------
# System Settings
# -----------------------------------------------------------------------------
DEBUG=false
LOG_LEVEL=INFO
